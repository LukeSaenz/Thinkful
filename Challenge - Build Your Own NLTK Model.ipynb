{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /Users/luke/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /anaconda3/lib/python3.6/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some options and load the data\n",
    "\n",
    "whitman_leaves = gutenberg.raw('whitman-leaves.txt')\n",
    "shakespeare_macbeth = gutenberg.raw('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitman_leaves = text_cleaner(whitman_leaves)\n",
    "shakespeare_macbeth = text_cleaner(shakespeare_macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Come, said my soul, Such verses for my Body let us write, (for we are one,) That should I after return, Or, long, long hence, in other spheres, There to some group of mates the chants resuming, (Tallying Earth's soil, trees, winds, tumultuous waves,) Ever with pleas'd smile I may\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a tweet-sized look at Whitman\n",
    "whitman_leaves[0:280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Actus Primus. Scoena Prima. Thunder and Lightning. Enter three Witches. 1. When shall we three meet againe? In Thunder, Lightning, or in Raine? 2. When the Hurley-burley's done, When the Battaile's lost, and wonne 3. That will be ere the set of Sunne 1. Where the place? 2. Vpon t\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same with Shakespeare\n",
    "shakespeare_macbeth[0:280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned works...\n",
    "nlp = spacy.load('en')\n",
    "whitman_doc = nlp(whitman_leaves)\n",
    "shakespeare_doc = nlp(shakespeare_macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Come, ,, said, my, soul, ,, Such, verses, for...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(That, should, I, after, return, ,, Or, ,, lon...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Ever, with, pleas'd, smile, I, may, keep, on,...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(I, here, and, now, Signing, for, Soul, and, B...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(One's)</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Come, ,, said, my, soul, ,, Such, verses, for...  Whitman\n",
       "1  (That, should, I, after, return, ,, Or, ,, lon...  Whitman\n",
       "2  (Ever, with, pleas'd, smile, I, may, keep, on,...  Whitman\n",
       "3  (I, here, and, now, Signing, for, Soul, and, B...  Whitman\n",
       "4                                            (One's)  Whitman"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "leaves_sents = [[sent, \"Whitman\"] for sent in whitman_doc.sents]\n",
    "macbeth_sents = [[sent, \"Shakespeare\"] for sent in shakespeare_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two works into one data frame.\n",
    "sentences = pd.DataFrame(leaves_sents + macbeth_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words. Augmented to exclude titles - \n",
    "# if not line.text.isupper()\n",
    "\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence'][::100]):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "whitmanwords = bag_of_words(whitman_doc)\n",
    "shakespearewords = bag_of_words(shakespeare_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(whitmanwords + shakespearewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lesse',\n",
       " 'taste',\n",
       " 'fetch',\n",
       " 'strongly',\n",
       " 'lecture',\n",
       " 'stirre',\n",
       " 'adhere',\n",
       " 'mouth',\n",
       " 'seat',\n",
       " 'impassive',\n",
       " 'region',\n",
       " 'wander',\n",
       " 'bird',\n",
       " '6',\n",
       " 'precedent',\n",
       " 'vast',\n",
       " 'missouri',\n",
       " 'lead',\n",
       " 'make',\n",
       " 'highly',\n",
       " 'print',\n",
       " 'branch',\n",
       " 'honor',\n",
       " 'yong',\n",
       " 'live',\n",
       " 'reuolt',\n",
       " 'sweaty',\n",
       " 'sexta',\n",
       " 'stabs',\n",
       " 'tangle',\n",
       " 'cheere',\n",
       " 'stock',\n",
       " 'wisdom',\n",
       " 'tear',\n",
       " 'silver',\n",
       " 'man',\n",
       " 'cawdor',\n",
       " 'clamor',\n",
       " 'onward',\n",
       " 'spiritual',\n",
       " 'afternoon',\n",
       " 'arrogant',\n",
       " 'mur',\n",
       " 'sake',\n",
       " 'after',\n",
       " 'always',\n",
       " 'houre',\n",
       " 'orbic',\n",
       " 'registr',\n",
       " 'thine',\n",
       " 'include',\n",
       " 'sleeper',\n",
       " 'damnation',\n",
       " 'seize',\n",
       " 'nimbly',\n",
       " 'wofull',\n",
       " 'appar',\n",
       " 'shaft',\n",
       " 'repeat',\n",
       " 'pulse',\n",
       " \"hang'd\",\n",
       " 'note',\n",
       " 'shoot',\n",
       " 'ayde',\n",
       " 'c.',\n",
       " 'dreadfull',\n",
       " 'chamber',\n",
       " 'increase',\n",
       " \"powr'd\",\n",
       " 'skinne',\n",
       " 'hayle',\n",
       " 'didst',\n",
       " 'woman',\n",
       " 'shut',\n",
       " 'patch',\n",
       " 'pleade',\n",
       " 'permit',\n",
       " 'neyther',\n",
       " 'guilt',\n",
       " 'guest',\n",
       " 'might',\n",
       " 'mighty',\n",
       " 'pennant',\n",
       " 'want',\n",
       " 'wing',\n",
       " 'briefely',\n",
       " 'ready',\n",
       " 'seeme',\n",
       " 'regard',\n",
       " 'perfectly',\n",
       " 'dis',\n",
       " 'treason',\n",
       " 'sin',\n",
       " 'slave',\n",
       " 'dyre',\n",
       " 'lampe',\n",
       " 'offspring',\n",
       " 'predominance',\n",
       " 'happiness',\n",
       " 'stealthy',\n",
       " 'applaud',\n",
       " 'faile',\n",
       " 'rage',\n",
       " 'himselfe',\n",
       " 'se',\n",
       " 'lose',\n",
       " 'depth',\n",
       " 'predecessor',\n",
       " 'aloft',\n",
       " \"awak'd\",\n",
       " 'saddle',\n",
       " 'bin',\n",
       " 'dull',\n",
       " 'bonfire',\n",
       " 'carows',\n",
       " 'limitted',\n",
       " 'sightlesse',\n",
       " 'compunctious',\n",
       " 'ouerthrowne',\n",
       " 'hark',\n",
       " 'hawk',\n",
       " 'custome',\n",
       " 'enrage',\n",
       " 'small',\n",
       " 'ore',\n",
       " 'account',\n",
       " 'vnusuall',\n",
       " 'tertia',\n",
       " 'whistle',\n",
       " 'glide',\n",
       " 'amazement',\n",
       " 'guard',\n",
       " 'wife',\n",
       " 'dumb',\n",
       " 'water',\n",
       " 'grain',\n",
       " 'spring',\n",
       " 'foot',\n",
       " 'boundless',\n",
       " 'hah',\n",
       " 'donalbaine',\n",
       " 'wearie',\n",
       " 'orchard',\n",
       " 'later',\n",
       " 'performe',\n",
       " 'none',\n",
       " 'sky',\n",
       " 'ease',\n",
       " 'saw',\n",
       " 'confront',\n",
       " 'heccats',\n",
       " 'beyond',\n",
       " 'friend',\n",
       " 'scale',\n",
       " 'perfection',\n",
       " 'support',\n",
       " 'satisfy',\n",
       " 'lord',\n",
       " 'curb',\n",
       " 'atlantic',\n",
       " 'even',\n",
       " 'eastern',\n",
       " 'yankee',\n",
       " 'something',\n",
       " 'height',\n",
       " 'haunt',\n",
       " 'democracy',\n",
       " 'shakes',\n",
       " 'mother',\n",
       " 'bible',\n",
       " 'steady',\n",
       " \"accomplish'd\",\n",
       " 'flit',\n",
       " 'ironical',\n",
       " 'church',\n",
       " \"suffer'd\",\n",
       " 'heauie',\n",
       " 'scotland',\n",
       " 'fold',\n",
       " 'proud',\n",
       " 'sorrowful',\n",
       " 'send',\n",
       " 'top',\n",
       " 'neck',\n",
       " 'army',\n",
       " 'draw',\n",
       " 'area',\n",
       " 'swimming',\n",
       " 'expression',\n",
       " 'corner',\n",
       " 'bid',\n",
       " 'mingle',\n",
       " 'diffuse',\n",
       " 'auouch',\n",
       " 'barke',\n",
       " 'flash',\n",
       " 'castles',\n",
       " 'lambe',\n",
       " 'famine',\n",
       " 'driver',\n",
       " 'calm',\n",
       " 'full',\n",
       " 'ang',\n",
       " 'saint',\n",
       " \"gather'd\",\n",
       " 'answer',\n",
       " 'quinta',\n",
       " 'people',\n",
       " 'feele',\n",
       " 'own',\n",
       " 'shroud',\n",
       " 'compt',\n",
       " 'emerge',\n",
       " 'forge',\n",
       " 'lilac',\n",
       " 'blacke',\n",
       " 'pavement',\n",
       " 'sorry',\n",
       " 'legges',\n",
       " 'maker',\n",
       " 'week',\n",
       " 'pond',\n",
       " 'fantasticall',\n",
       " 'hardly',\n",
       " 'solid',\n",
       " 'hence',\n",
       " 'bugle',\n",
       " 'commit',\n",
       " 'course',\n",
       " 'pass',\n",
       " 'learn',\n",
       " 'near',\n",
       " 'book',\n",
       " 'hoa',\n",
       " 'dew',\n",
       " 'confusion',\n",
       " 'saue',\n",
       " 'drench',\n",
       " 'proof',\n",
       " 'away',\n",
       " 'accesse',\n",
       " 'title',\n",
       " 'euerlasting',\n",
       " 'sailing',\n",
       " 'condition',\n",
       " 'serpent',\n",
       " 'afeard',\n",
       " 'returne',\n",
       " 'leaf',\n",
       " 'exhibit',\n",
       " 'therefore',\n",
       " 'camerado',\n",
       " 'swarme',\n",
       " 'ask',\n",
       " 'ecstatic',\n",
       " \"ow'd\",\n",
       " 'heart',\n",
       " 'state',\n",
       " 'tost',\n",
       " 'mind',\n",
       " 'milke',\n",
       " 'camp',\n",
       " 'perfect',\n",
       " 'pick',\n",
       " 'entrance',\n",
       " 'certaine',\n",
       " 'hose',\n",
       " 'groomes',\n",
       " 'trees',\n",
       " 'hast',\n",
       " 'envelop',\n",
       " 'desire',\n",
       " 'personal',\n",
       " 'leg',\n",
       " \"lac'd\",\n",
       " 'countreyes',\n",
       " 'dreyne',\n",
       " 'capitol',\n",
       " 'hau',\n",
       " 'issue',\n",
       " 'perchance',\n",
       " 'wilde',\n",
       " 'continually',\n",
       " 'spot',\n",
       " \"might'st\",\n",
       " \"honor'd\",\n",
       " 'alive',\n",
       " 'sowre',\n",
       " 'sparrowes',\n",
       " 'car',\n",
       " 'loss',\n",
       " 'yeeld',\n",
       " 'rebell',\n",
       " 'purport',\n",
       " 'lad',\n",
       " 'candle',\n",
       " \"smother'd\",\n",
       " 'westerne',\n",
       " 'unnamed',\n",
       " 'and',\n",
       " 'eye',\n",
       " 'poster',\n",
       " 'execution',\n",
       " \"gash'd\",\n",
       " 'bare',\n",
       " 'darkness',\n",
       " 'tie',\n",
       " 'order',\n",
       " 'nothing',\n",
       " \"vnwip'd\",\n",
       " 'do',\n",
       " 'theft',\n",
       " 'ebb',\n",
       " 'most',\n",
       " \"neu'r\",\n",
       " 'inside',\n",
       " 'crye',\n",
       " 'mistress',\n",
       " 'promise',\n",
       " 'anchor',\n",
       " 'officer',\n",
       " 'filthie',\n",
       " 'groan',\n",
       " 'aught',\n",
       " '13',\n",
       " 'ministers',\n",
       " 'chamberlaine',\n",
       " 'byrnane',\n",
       " 'latent',\n",
       " 'audit',\n",
       " 'dwelling',\n",
       " 'count',\n",
       " 'vaulting',\n",
       " 'bene',\n",
       " 'farmer',\n",
       " 'meane',\n",
       " 'sences',\n",
       " 'work',\n",
       " 'smeare',\n",
       " 'minute',\n",
       " 'oracle',\n",
       " 'plenteous',\n",
       " 'knocking',\n",
       " 'treat',\n",
       " 'general',\n",
       " 'search',\n",
       " 'africa',\n",
       " 'thirty',\n",
       " \"thou'rt\",\n",
       " 'only',\n",
       " 'sink',\n",
       " 'turne',\n",
       " 'instructions',\n",
       " 'wear',\n",
       " 'drunke',\n",
       " 'especially',\n",
       " 'neer',\n",
       " 'stopt',\n",
       " 'weare',\n",
       " 'barlet',\n",
       " 'chair',\n",
       " 'god',\n",
       " 'teacher',\n",
       " 'ghost',\n",
       " 'crown',\n",
       " 'rid',\n",
       " 'bee',\n",
       " 'leau',\n",
       " \"thould'st\",\n",
       " 'dash',\n",
       " 'manly',\n",
       " 'garments',\n",
       " 'scruple',\n",
       " 'under',\n",
       " 'tight',\n",
       " 'sufficient',\n",
       " 'hidden',\n",
       " 'manhood',\n",
       " 'hadst',\n",
       " 'supreme',\n",
       " 'rifle',\n",
       " \"envelop'd\",\n",
       " 'distance',\n",
       " 'writing',\n",
       " 'gorgon',\n",
       " 'affaire',\n",
       " \"inform'd\",\n",
       " 'true',\n",
       " 'happyer',\n",
       " 'egg',\n",
       " 'blank',\n",
       " 'how',\n",
       " 'beard',\n",
       " 'contribute',\n",
       " 'disease',\n",
       " 'law',\n",
       " 'beat',\n",
       " 'shot',\n",
       " 'moss',\n",
       " 'borrow',\n",
       " 'spirit',\n",
       " 'grand',\n",
       " 'texas',\n",
       " 'shift',\n",
       " 'silence',\n",
       " 'shadowy',\n",
       " 'protest',\n",
       " 'sleeue',\n",
       " 'roll',\n",
       " 'ignorant',\n",
       " 'scales',\n",
       " 'anguish',\n",
       " 'fulnesse',\n",
       " 'comparison',\n",
       " \"pleas'd\",\n",
       " 'suspect',\n",
       " 'desperate',\n",
       " 'dispaire',\n",
       " 'craues',\n",
       " 'identity',\n",
       " \"confirm'd\",\n",
       " 'mortalitie',\n",
       " 'walke',\n",
       " 'special',\n",
       " 'around',\n",
       " 'paye',\n",
       " 'start',\n",
       " '4',\n",
       " \"you'l\",\n",
       " 'prospect',\n",
       " 'faced',\n",
       " 'england',\n",
       " 'slaine',\n",
       " 'trammell',\n",
       " 'custom',\n",
       " 'memorize',\n",
       " 'death',\n",
       " 'commendation',\n",
       " 'heave',\n",
       " 'heaven',\n",
       " 'seemeth',\n",
       " 'poore',\n",
       " \"vnprepar'd\",\n",
       " 'lane',\n",
       " 'deuils',\n",
       " 'gift',\n",
       " 'norweyan',\n",
       " 'time',\n",
       " 'railroad',\n",
       " 'gin',\n",
       " 'town',\n",
       " 'brave',\n",
       " 'haze',\n",
       " 'beguile',\n",
       " 'babe',\n",
       " 'pay',\n",
       " 'darke',\n",
       " 'smoake',\n",
       " 'subborn',\n",
       " 'fierce',\n",
       " 'easie',\n",
       " 'canst',\n",
       " 'offer',\n",
       " 'incessant',\n",
       " 'haue',\n",
       " 'withall',\n",
       " 'troop',\n",
       " 'hurry',\n",
       " 'square',\n",
       " 'ayre',\n",
       " \"o'that\",\n",
       " 'messenger',\n",
       " 'gain',\n",
       " 'gorgeous',\n",
       " 'distract',\n",
       " 'without',\n",
       " 'bride',\n",
       " 'rice',\n",
       " 'abuse',\n",
       " 'iumpe',\n",
       " 'heire',\n",
       " 'chimneys',\n",
       " 'fret',\n",
       " 'tan',\n",
       " 'signal',\n",
       " 'leysure',\n",
       " 'game',\n",
       " 'hip',\n",
       " 'faith',\n",
       " 'prouok',\n",
       " 'rauish',\n",
       " 'oxen',\n",
       " 'summer',\n",
       " 'dr',\n",
       " 'room',\n",
       " 'surely',\n",
       " 'fuse',\n",
       " 'invention',\n",
       " 'assume',\n",
       " 'centinell',\n",
       " 'blesse',\n",
       " 'drunkard',\n",
       " 'bestow',\n",
       " 'shallow',\n",
       " 'appear',\n",
       " 'vine',\n",
       " 'skull',\n",
       " 'land',\n",
       " 'daybreak',\n",
       " 'thus',\n",
       " 'feature',\n",
       " 'escape',\n",
       " 'wrapt',\n",
       " 'function',\n",
       " \"tongu'd\",\n",
       " 'taking',\n",
       " 'gentle',\n",
       " 'poverty',\n",
       " 'banquo',\n",
       " 'shelf',\n",
       " 'comrade',\n",
       " 'jet',\n",
       " 'wondrous',\n",
       " \"determin'd\",\n",
       " 'powre',\n",
       " 'natures',\n",
       " 'kindnesse',\n",
       " 'stout',\n",
       " 'whom',\n",
       " 'o',\n",
       " \"'s\",\n",
       " 'pain',\n",
       " 'fatall',\n",
       " 'colour',\n",
       " 'serve',\n",
       " 'envernes',\n",
       " 'charm',\n",
       " 'ago',\n",
       " 'spread',\n",
       " 'shirt',\n",
       " 'vnprouok',\n",
       " 'prays',\n",
       " 'blood',\n",
       " 'talk',\n",
       " 'flaunt',\n",
       " 'thinke',\n",
       " 'holily',\n",
       " 'weale',\n",
       " 'brooklyn',\n",
       " 'poem',\n",
       " 'corporall',\n",
       " 'dun',\n",
       " 'alike',\n",
       " 'inhabitant',\n",
       " 'musician',\n",
       " 'brandisht',\n",
       " 'gather',\n",
       " 'cycle',\n",
       " 'ribbes',\n",
       " 'toe',\n",
       " 'choose',\n",
       " 'where',\n",
       " \"veil'd\",\n",
       " 'instruments',\n",
       " 'encompass',\n",
       " 'particular',\n",
       " 'victorie',\n",
       " 'splash',\n",
       " 'source',\n",
       " 'divine',\n",
       " 'living',\n",
       " 'passionately',\n",
       " 'dusk',\n",
       " 'suddenly',\n",
       " 'go',\n",
       " 'scaffold',\n",
       " 'theory',\n",
       " 'earnest',\n",
       " 'beneath',\n",
       " \"learn'd\",\n",
       " \"fix'd\",\n",
       " 'key',\n",
       " 'eidolon',\n",
       " 'rapt',\n",
       " 'syllable',\n",
       " 'rope',\n",
       " 'receit',\n",
       " 'suspition',\n",
       " 'proofe',\n",
       " 'side',\n",
       " 'pioneer',\n",
       " 'sob',\n",
       " 'wreck',\n",
       " 'launch',\n",
       " 'enemy',\n",
       " 'meal',\n",
       " \"crown'd\",\n",
       " 'entire',\n",
       " 'boy',\n",
       " 'drift',\n",
       " 'drink',\n",
       " 'farewell',\n",
       " 'haste',\n",
       " 'beleefe',\n",
       " 'vndaunted',\n",
       " 'use',\n",
       " 'healthy',\n",
       " 'businesse',\n",
       " 'prouoke',\n",
       " 'drowne',\n",
       " 'glance',\n",
       " 'hat',\n",
       " 'resistless',\n",
       " 'take',\n",
       " 'moone',\n",
       " 'bearded',\n",
       " 'lees',\n",
       " '7',\n",
       " 'grandson',\n",
       " 'seas',\n",
       " 'study',\n",
       " 'preparation',\n",
       " 'tyrant',\n",
       " 'thunder',\n",
       " 'wealth',\n",
       " 'royall',\n",
       " 'reuenge',\n",
       " 'dwindle',\n",
       " 'moment',\n",
       " 'kiss',\n",
       " 'skie',\n",
       " 'incarnardine',\n",
       " 'tiger',\n",
       " 'dirt',\n",
       " 'chaste',\n",
       " 'project',\n",
       " 'renowne',\n",
       " 'exposure',\n",
       " 'sprout',\n",
       " 'indifferent',\n",
       " 'countenance',\n",
       " 'liu',\n",
       " 'captiuitie',\n",
       " 'file',\n",
       " \"charg'd\",\n",
       " \"speak'st\",\n",
       " 'wilt',\n",
       " 'forest',\n",
       " 'greet',\n",
       " 'bat',\n",
       " 'main',\n",
       " 'atmosphere',\n",
       " 'quarry',\n",
       " 'ingredience',\n",
       " 'slipt',\n",
       " 'march',\n",
       " 'many',\n",
       " 'drumme',\n",
       " 'space',\n",
       " 'absorb',\n",
       " 'post',\n",
       " 'receiue',\n",
       " \"badg'd\",\n",
       " 'age',\n",
       " 'chestnuts',\n",
       " 'graces',\n",
       " 'receive',\n",
       " 'plight',\n",
       " 'tribe',\n",
       " 'venom',\n",
       " \"watch'd\",\n",
       " 'weed',\n",
       " \"temp'rate\",\n",
       " 'mast',\n",
       " 'flag',\n",
       " 'taylor',\n",
       " 'harrold',\n",
       " 'french',\n",
       " 'heele',\n",
       " 'embody',\n",
       " 'brother',\n",
       " 'motion',\n",
       " 'affection',\n",
       " 'civilization',\n",
       " 'loving',\n",
       " 'cool',\n",
       " 'bud',\n",
       " 'tumultuous',\n",
       " 'faulcon',\n",
       " 'indirection',\n",
       " 'stands',\n",
       " 'machinery',\n",
       " 'lover',\n",
       " 'experience',\n",
       " 'add',\n",
       " 'depart',\n",
       " 'secret',\n",
       " 'past',\n",
       " 'superior',\n",
       " 'distant',\n",
       " 'dunsinane',\n",
       " 'hero',\n",
       " 'inuenter',\n",
       " 'stage',\n",
       " 'trill',\n",
       " '10',\n",
       " 'pall',\n",
       " 'that',\n",
       " 'vnto',\n",
       " 'overhead',\n",
       " 'maiesty',\n",
       " 'skin',\n",
       " 'round',\n",
       " \"drugg'd\",\n",
       " 'miracle',\n",
       " 'traitors',\n",
       " 'dear',\n",
       " 'landscape',\n",
       " 'far',\n",
       " 'pretence',\n",
       " 'mad',\n",
       " \"diseas'd\",\n",
       " 'dusky',\n",
       " 'fauour',\n",
       " 'destiny',\n",
       " 'desk',\n",
       " 'country',\n",
       " 'gashes',\n",
       " 'cannons',\n",
       " 'gods',\n",
       " 'campaign',\n",
       " 'frailties',\n",
       " 'sunrise',\n",
       " 'adieu',\n",
       " 'raise',\n",
       " 'tent',\n",
       " 'minister',\n",
       " 'sell',\n",
       " 'kings',\n",
       " 'master',\n",
       " 'prouid',\n",
       " 'recitative',\n",
       " 'besides',\n",
       " 'reigne',\n",
       " 'avail',\n",
       " 'electric',\n",
       " 'curve',\n",
       " 'images',\n",
       " 'memorie',\n",
       " 'history',\n",
       " \"dream'd\",\n",
       " 'list',\n",
       " 'coffin',\n",
       " 'torch',\n",
       " 'vanish',\n",
       " 'laborer',\n",
       " 'iourney',\n",
       " 'suggestion',\n",
       " 'move',\n",
       " 'whether',\n",
       " 'commerce',\n",
       " 'sewer',\n",
       " 'clear',\n",
       " 'minions',\n",
       " 'youth',\n",
       " 'reach',\n",
       " 'tend',\n",
       " 'neuer',\n",
       " 'laughter',\n",
       " 'broadway',\n",
       " 'rock',\n",
       " 'whirl',\n",
       " 'seywards',\n",
       " 'purpose',\n",
       " 'star',\n",
       " 'owe',\n",
       " 'ten',\n",
       " 'wounds',\n",
       " 'strange',\n",
       " 'paradise',\n",
       " 'quarta',\n",
       " 'language',\n",
       " 'wit',\n",
       " 'subtle',\n",
       " 'iutty',\n",
       " 'permanent',\n",
       " 'fleance',\n",
       " 'outward',\n",
       " 'real',\n",
       " 'way',\n",
       " 'limb',\n",
       " 'force',\n",
       " 'stately',\n",
       " 'join',\n",
       " 'orator',\n",
       " 'greatnesse',\n",
       " 'flye',\n",
       " 'equiuocator',\n",
       " 'system',\n",
       " 'shop',\n",
       " 'paine',\n",
       " 'torches',\n",
       " 'fauor',\n",
       " \"may'st\",\n",
       " 'addition',\n",
       " 'y',\n",
       " \"take't\",\n",
       " 'golden',\n",
       " 'ah',\n",
       " 'haire',\n",
       " 'confounds',\n",
       " 'sight',\n",
       " 'soft',\n",
       " 'nought',\n",
       " 'cloud',\n",
       " 'poure',\n",
       " 'glosse',\n",
       " 'effect',\n",
       " 'sword',\n",
       " 'hauing',\n",
       " 'track',\n",
       " 'patience',\n",
       " 'endure',\n",
       " 'bridge',\n",
       " \"vpon't\",\n",
       " 'descend',\n",
       " 'evening',\n",
       " 'substance',\n",
       " 'soules',\n",
       " \"wither'd\",\n",
       " 'surfeted',\n",
       " 'hall',\n",
       " 'cold',\n",
       " 'graine',\n",
       " 'crash',\n",
       " 'ha',\n",
       " 'shooke',\n",
       " 'monument',\n",
       " 'news',\n",
       " 'homeward',\n",
       " 'eyth',\n",
       " 'lords',\n",
       " 'shout',\n",
       " 'urge',\n",
       " 'hair',\n",
       " \"follow'd\",\n",
       " 'naue',\n",
       " 'animal',\n",
       " 'primal',\n",
       " 'byrnan',\n",
       " 'artist',\n",
       " 'paralell',\n",
       " 'foole',\n",
       " 'adde',\n",
       " 'justice',\n",
       " 'aduise',\n",
       " 'bad',\n",
       " 'axe',\n",
       " 'tall',\n",
       " 'whence',\n",
       " 'toward',\n",
       " 'euents',\n",
       " 'sawce',\n",
       " 'contain',\n",
       " \"vs'd\",\n",
       " 'soldier',\n",
       " 'scena',\n",
       " 'trunk',\n",
       " 'rest',\n",
       " 'selue',\n",
       " \"shriek'd\",\n",
       " 'read',\n",
       " 'naturall',\n",
       " 'conceiue',\n",
       " 'dearest',\n",
       " \"lipp'd\",\n",
       " 'warning',\n",
       " 'duncane',\n",
       " 'grave',\n",
       " 'herd',\n",
       " 'office',\n",
       " 'behold',\n",
       " 'care',\n",
       " 'equality',\n",
       " 'finally',\n",
       " 'winde',\n",
       " 'soone',\n",
       " 'beam',\n",
       " 'beene',\n",
       " 'poet',\n",
       " 'lenox',\n",
       " 'dancer',\n",
       " 'wandering',\n",
       " 'comm',\n",
       " 'fitful',\n",
       " 'enfold',\n",
       " \"hors'd\",\n",
       " 'ceaseless',\n",
       " 'breathing',\n",
       " 'i.',\n",
       " 'noise',\n",
       " 'sugar',\n",
       " 'group',\n",
       " 'log',\n",
       " 'tenderly',\n",
       " 'drape',\n",
       " 'tremble',\n",
       " 'fight',\n",
       " \"kill'd\",\n",
       " 'ayme',\n",
       " 'meere',\n",
       " 'some',\n",
       " 'blanket',\n",
       " 'greek',\n",
       " 'seed',\n",
       " 'entry',\n",
       " 'vnruly',\n",
       " 'chastise',\n",
       " 'swinish',\n",
       " 'soothe',\n",
       " 'gaue',\n",
       " 'steel',\n",
       " 'tenor',\n",
       " 'wrist',\n",
       " 'poor',\n",
       " 'birnane',\n",
       " 'end',\n",
       " 'foorth',\n",
       " 'evil',\n",
       " 'horrid',\n",
       " 'pen',\n",
       " 'prosperous',\n",
       " 'gracious',\n",
       " 'dissatisfied',\n",
       " 'butcher',\n",
       " 'farwell',\n",
       " 'moue',\n",
       " 'temple',\n",
       " 'perpetual',\n",
       " 'rebellious',\n",
       " 'pour',\n",
       " 'impalpable',\n",
       " 'interminable',\n",
       " 'towr',\n",
       " 'immortal',\n",
       " 'choke',\n",
       " 'kisse',\n",
       " 'fogge',\n",
       " 'cavalry',\n",
       " 'antique',\n",
       " 'wrackt',\n",
       " 'valour',\n",
       " 'suppose',\n",
       " 'sundown',\n",
       " 'smack',\n",
       " 'fisherman',\n",
       " 'long',\n",
       " 'know',\n",
       " 'prairie',\n",
       " 'prove',\n",
       " 'graf',\n",
       " 'thin',\n",
       " 'out',\n",
       " 'eve',\n",
       " 'perceive',\n",
       " 'coast',\n",
       " 'timely',\n",
       " 'haruest',\n",
       " 'quality',\n",
       " 'bark',\n",
       " 'universe',\n",
       " 'see',\n",
       " 'more',\n",
       " 'son',\n",
       " 'could',\n",
       " 'constancie',\n",
       " 'better',\n",
       " 'wheel',\n",
       " 'greeting',\n",
       " 'eternity',\n",
       " 'gently',\n",
       " 'feudal',\n",
       " \"know't\",\n",
       " 'set',\n",
       " 'what',\n",
       " 'tongue',\n",
       " 'mould',\n",
       " 'intent',\n",
       " 'soueraigne',\n",
       " 'essence',\n",
       " 'innocent',\n",
       " 'eligible',\n",
       " \"murther'd\",\n",
       " 'human',\n",
       " 'dead',\n",
       " 'awhile',\n",
       " 'pastoral',\n",
       " 'hoarse',\n",
       " 'hec',\n",
       " 'meet',\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n"
     ]
    }
   ],
   "source": [
    "# create our data with features...\n",
    "word_counts = bow_features(sentences, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding another feature using TextBlob to assess sentiment\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "word_counts['text_sentence_sentiment_polarity'] = word_counts['text_sentence'].apply(str).apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "word_counts['text_sentence_sentiment_subjectivity'] = word_counts['text_sentence'].apply(str).apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts['text_sentence_avg_word_length'] = word_counts['text_sentence'].apply(str).apply(lambda x: x.split())\n",
    "word_counts['text_sentence_avg_word_length'] = word_counts['text_sentence_avg_word_length'].apply(lambda words: sum(len(word) for word in words) / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesse</th>\n",
       "      <th>taste</th>\n",
       "      <th>fetch</th>\n",
       "      <th>strongly</th>\n",
       "      <th>lecture</th>\n",
       "      <th>stirre</th>\n",
       "      <th>adhere</th>\n",
       "      <th>mouth</th>\n",
       "      <th>seat</th>\n",
       "      <th>impassive</th>\n",
       "      <th>...</th>\n",
       "      <th>continent</th>\n",
       "      <th>truths</th>\n",
       "      <th>stream</th>\n",
       "      <th>sit</th>\n",
       "      <th>macbeths</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>text_sentence_sentiment_polarity</th>\n",
       "      <th>text_sentence_sentiment_subjectivity</th>\n",
       "      <th>text_sentence_avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Come, ,, said, my, soul, ,, Such, verses, for...</td>\n",
       "      <td>Whitman</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(That, should, I, after, return, ,, Or, ,, lon...</td>\n",
       "      <td>Whitman</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>5.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Ever, with, pleas'd, smile, I, may, keep, on,...</td>\n",
       "      <td>Whitman</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>4.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, here, and, now, Signing, for, Soul, and, B...</td>\n",
       "      <td>Whitman</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(One's)</td>\n",
       "      <td>Whitman</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3447 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  lesse taste fetch strongly lecture stirre adhere mouth seat impassive  \\\n",
       "0     0     0     0        0       0      0      0     0    0         0   \n",
       "1     0     0     0        0       0      0      0     0    0         0   \n",
       "2     0     0     0        0       0      0      0     0    0         0   \n",
       "3     0     0     0        0       0      0      0     0    0         0   \n",
       "4     0     0     0        0       0      0      0     0    0         0   \n",
       "\n",
       "               ...              continent truths stream sit macbeths  \\\n",
       "0              ...                      0      0      0   0        0   \n",
       "1              ...                      0      0      0   0        0   \n",
       "2              ...                      0      0      1   0        0   \n",
       "3              ...                      0      0      0   0        0   \n",
       "4              ...                      0      0      0   0        0   \n",
       "\n",
       "                                       text_sentence text_source  \\\n",
       "0  (Come, ,, said, my, soul, ,, Such, verses, for...     Whitman   \n",
       "1  (That, should, I, after, return, ,, Or, ,, lon...     Whitman   \n",
       "2  (Ever, with, pleas'd, smile, I, may, keep, on,...     Whitman   \n",
       "3  (I, here, and, now, Signing, for, Soul, and, B...     Whitman   \n",
       "4                                            (One's)     Whitman   \n",
       "\n",
       "  text_sentence_sentiment_polarity text_sentence_sentiment_subjectivity  \\\n",
       "0                            0.000                             0.500000   \n",
       "1                           -0.075                             0.391667   \n",
       "2                            0.275                             0.216667   \n",
       "3                            0.000                             0.000000   \n",
       "4                            0.000                             0.000000   \n",
       "\n",
       "  text_sentence_avg_word_length  \n",
       "0                      3.750000  \n",
       "1                      5.250000  \n",
       "2                      4.058824  \n",
       "3                      3.588235  \n",
       "4                      5.000000  \n",
       "\n",
       "[5 rows x 3447 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4569, 3445) (4569,)\n",
      "Training set score: 0.744364193477785\n",
      "\n",
      "Test set score: 0.7535280603872662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1  747]\n",
      " [   4 2295]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Shakespeare       0.20      0.00      0.00       748\n",
      "    Whitman       0.75      1.00      0.86      2299\n",
      "\n",
      "avg / total       0.62      0.75      0.65      3047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4569, 3445) (4569,)\n",
      "Training set score: 0.7463339899321515\n",
      "\n",
      "Test set score: 0.7545126353790613\n"
     ]
    }
   ],
   "source": [
    "# Using SVM as an alternative\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "                intercept_scaling=1, loss='hinge', max_iter=1000,\n",
    "                multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "                verbose=0)\n",
    "\n",
    "train = svm.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', svm.score(X_train, y_train))\n",
    "print('\\nTest set score:', svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  748]\n",
      " [   0 2299]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Shakespeare       0.00      0.00      0.00       748\n",
      "    Whitman       0.75      1.00      0.86      2299\n",
      "\n",
      "avg / total       0.57      0.75      0.65      3047\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred = svm.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7609980302035456\n",
      "\n",
      "Test set score: 0.7505743354118806\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  47  701]\n",
      " [  59 2240]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Shakespeare       0.44      0.06      0.11       748\n",
      "    Whitman       0.76      0.97      0.85      2299\n",
      "\n",
      "avg / total       0.68      0.75      0.67      3047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "It would seem that gradient boosting is performing the best of the three models above, particularly with regard to correctly classifying Shakespeare. Even a 0.11 f1-score is significantly better than a 0.0 scores it garnered in SVM and Logistic Regression. \n",
    "\n",
    "Furthermore, judging by the confusion matrices, particularly in regression and SVM, it seems that we really aren't doing much Shakespeare guessing.\n",
    "\n",
    "Although we're coming out of our tests looking at accuracy scores in the mid-70s, one must really question how good a measure accuracy is when a class imbalance such as the one above exists. For example, there are nearly 3 times as many instances of Whitman as there are Shakespeare, so guessing Whitman 100% of the time would lead to near 75% accuracy, which doesn't effectively evaluate the poor performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
